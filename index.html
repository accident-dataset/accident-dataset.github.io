<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="Towards Vision Zero: The TUM Traffic Accid3nD Dataset">
    <meta property="og:title" content="Towards Vision Zero: The TUM Traffic Accid3nD Dataset"/>
    <meta property="og:description" content="Towards Vision Zero: The TUM Traffic Accid3nD Dataset"/>
    <meta property="og:url" content="https://accident-dataset.github.io/"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="Towards Vision Zero: The TUM Traffic Accid3nD Dataset">
    <meta name="twitter:description" content="Towards Vision Zero: The TUM Traffic Accid3nD Dataset">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="Dataset">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Towards Vision Zero: The Accid3nD Dataset</title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-4 publication-title">Towards Vision Zero: The TUM Traffic Accid3nD Dataset</h1>
                    <!--                    <div class="is-size-5 publication-authors">-->
                    <!--                    </div>-->

                    <!--                    <div class="is-size-5 publication-authors">-->
                    <!--                        &lt;!&ndash; <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> &ndash;&gt;-->
                    <!--                    </div>-->

                    <div class="column has-text-centered">
                        <div class="publication-links">
                              <span class="link-block">
                              <a href="https://arxiv.org/abs/2503.12095" target="_blank"
                                 class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Paper</span>
                              </a>
                            </span>

                            <!-- Github link -->
                            <span class="link-block">
                                    <a href="https://github.com/accident-dataset/AccidentDet3D" target="_blank"
                                       class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Model</span>
                                    </a>
                                </span>


                            <!-- Dataset link -->
                            <span class="link-block">
                                    <a href="https://nx21496.your-storageshare.de/s/Zd4rwsMwwXMaXiK/download"
                                       target="_blank" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-database"></i>
                                        </span>
                                        <span>Dataset</span>
                                    </a>
                                </span>

                            <!-- Github link -->
                            <span class="link-block">
                                    <a href="https://github.com/walzimmer/3d-bat" target="_blank"
                                       class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Labeling Tool</span>
                                    </a>
                                </span>

                            <!-- Github link -->
                            <span class="link-block">
                                    <a href="https://github.com/tum-traffic-dataset/tum-traffic-dataset-dev-kit"
                                       target="_blank" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Development Kit</span>
                                    </a>
                                </span>


                            <!-- ArXiv abstract Link -->
                            <!-- <span class="link-block">
                          <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="ai ai-arxiv"></i>
                          </span>
                          <span>arXiv</span>
                          </a>
                        </span> -->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                <img src="static/images/accidents_title_cropped.svg" alt="infrastructure_sensors"
                     style="border: 2px solid #000000;"/>
                <h2 class="subtitle">
                    <b>Visualization of the raw TUM Traffic Accid3nD dataset.</b> Accidents are recorded from roadside cameras on the
                    A9 Digital Test Field for Autonomous Driving in Munich, Germany. The dataset includes scenes with collisions and overturning
                    vehicles. Some vehicles are catching fire.
                </h2>
            </div>
        </div>
    </div>
</section>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <video style="border:2px solid black;" poster="" autoplay controls muted loop height="100%">
                <source src="static/videos/grid_6x8_scaled.mp4" type="video/mp4">
            </video>
        </div>
    </div>
</section>

<!--<section class="hero is-small is-light">-->
<!--    <div class="hero-body">-->
<!--        <div class="container">-->
<!--            <h2 class="title is-3">Dataset Labeling</h2>-->
<!--            <video poster="" autoplay controls muted loop height="100%">-->
<!--                <source src="static/videos/timelapse_labeling_fast.mp4" type="video/mp4">-->
<!--            </video>-->
<!--        </div>-->
<!--    </div>-->
<!--</section>-->

<!-- Overview  -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Overview</h2>
                <div class="content">
                    <strong>The TUM Traffic Accid3nD (TUMTraf-Accid3nD)</strong> dataset is the first high-quality real-world accident
                    <strong>dataset</strong>
                    for the 3D object detection, 3D segmentation, 3D tracking, 3D trajectory prediction and 3D accident detection
                    task in autonomous driving.<br><br>
                    It contains:
                    <ul>
                        <li>data collected by <strong>5 sensors</strong> (cameras and LiDARs) simultaneously from
                            onboard and roadside sensors.
                        </li>
                        <li><strong>111,945</strong> labeled frames of vehicle crashes at high-speed driving.
                        </li>
                        <li><strong>2,634,233</strong> labeled 3D bounding boxes, instance segmentation masks, 2D
                            bounding boxes with track IDs.
                        </li>
                        <li>Real accidents including vehicle rollovers, vehicles catching fire and collision events.
                        </li>
                        <li><strong>HD map</strong> of the highway.</li>
                        <li>Labels in <strong>OpenLABEL</strong> standard.</li>
                        <li>A dataset <strong>development kit</strong> to load, preprocess, visualize, convert labels,
                            and to evaluate accident detection models.
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End overview -->

<section class="section hero">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Even though a significant amount of work has been done to increase the safety of transportation
                        networks, accidents still occur regularly. They must be understood as an unavoidable and
                        sporadic outcome of traffic networks. No public dataset contains real-world accidents recorded
                        from roadside sensors. We present the TUM Traffic Accid3nD (TUMTraf-Accid3nD) dataset, a collection of real-world highway
                        accidents in different weather and lighting conditions. It contains vehicle crashes at
                        high-speed driving with 2,634,233 labeled 3D bounding boxes, instance masks, and 2D bounding
                        boxes with track IDs. In total, the dataset contains 111,945 labeled frames recorded from four
                        roadside cameras and LiDARs at 25 Hz. The dataset contains six object classes and is provided in
                        the OpenLABEL format. We propose an accident detection model that combines a rule-based approach
                        with a learning-based one. Experiments and ablation studies on our dataset show the robustness
                        of our proposed method. The dataset, model, and code are publicly available on our project
                        website.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Sensor setup  -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Sensor Setup</h2>
                <div class="content is-align-content-start">
                    The following roadside sensors were used:
                    <ul>
                        <li>4x <strong>Basler</strong> ace acA1920-50gc, 1920×1200, Sony IMX174 with 16 mm and 50 mm
                            lenses, frame rate: 50 Hz
                        </li>
                        <li>1x <strong>Valeo</strong> LiDAR (SCALA B2), 16 vertical layers, 133° horiz. FOV, 0.125° x
                            0.6° angular resolution, 200 m range (@80% reflectivity), frame rate: 25 Hz
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End sensor setup -->

<!-- infrastructure sensors -->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                <img src="static/images/a9_test_field.svg" alt="infrastructure_sensors"
                     style="border:2px solid black;"/>
                <h2 class="subtitle">
                    <b>Overview of the A9 Digital Test Field.</b> The blue sensor stations (gantry bridges S040 and S050) on the highway were used to record the data with four roadside cameras, four radars, and one LiDAR.
                </h2>
                <center>
                    <img src="static/images/s050_cropped.svg" alt="infrastructure_sensors"
                         style="border:2px solid black;"/>
                </center>
                <h2 class="subtitle">
                    <b>Visualization of roadside sensors</b> used to record our TUM Traffic Accid3nD Dataset from roadside
                    infrastructure perspective.
                </h2>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                <h2 class="title is-3">Architecture</h2>
                <img src="static/images/pipeline_cropped.svg" alt="infrastructure_sensors"
                     style="border:2px solid black;"/>
                <h2 class="subtitle">
                    <b>Accident detection pipeline.</b> We use advanced 3D perception techniques and multi-sensor data
                    fusion to create a real-time
                    digital twin of the traffic. Starting with raw camera images, the framework first performs 3D object
                    detection using MonoDet3D to
                    identify and localize vehicles in three dimensions. Following detection, Poly-MOT tracking is
                    applied to maintain continuity across
                    frames, while sensor data fusion combines inputs from four roadside cameras and four radars. The
                    digital twin is then used in two
                    accident detection modules: 1) The <b>Rule-based Accident Detection</b> module extracts features
                    such as lane IDs, distance matrices, and
                    velocities, identifying potential accidents through predefined maneuver detection rules. 2) The <b>Learning-based
                    Accident Detection</b>
                    module employs a YOLOv8 object detector, trained on a custom dataset, to detect accident events. The
                    final output includes the object’s
                    location, confidence score, class, velocity, and detected scenario or maneuver.
                </h2>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                <h2 class="title is-3">TUM Traffic Accid3nD Dataset</h2>
                <img src="static/images/accidents_labeled_cropped.svg" alt="infrastructure_sensors"
                     style="border:2px solid black;"/>
                <h2 class="subtitle">
                    <b>Visualization of the labeled TUMTraf-Accid3nD dataset</b> with 3D box annotations, instance masks, track
                    IDs, and trajectories.
                    Accidents are recorded from roadside cameras on a test bed for autonomous driving. The dataset
                    includes scenes with collisions and
                    overturning vehicles. Some vehicles are catching fire.
                </h2>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                <h2 class="title is-3">Statistics</h2>
                <center>
                    <img src="static/images/bar_chart_class_occurrences_all_drives.svg" alt="infrastructure_sensors"/>
                    <h2 class="subtitle has-text-centered">
                        Distribution of labeled object classes in the TUMTraf-Accid3nD dataset.
                    </h2>
                    <img src="static/images/bar_chart_avg_track_lengths_all_drives.svg" alt="infrastructure_sensors"/>
                    <h2 class="subtitle has-text-centered">
                        Average and max. track lengths of all labeled object classes.
                    </h2>
                    <img src="static/images/histogram_distances.svg" alt="infrastructure_sensors"/>
                    <h2 class="subtitle has-text-centered">
                        Histogram of labeling distances.
                    </h2>
                    <img src="static/images/histogram_objects_in_frame.svg" alt="infrastructure_sensors"/>
                    <h2 class="subtitle has-text-centered">
                        Histogram of the number of 3D box labels.
                    </h2>
                    <img src="static/images/histogram_track_lengths.svg" alt="infrastructure_sensors"/>
                    <h2 class="subtitle has-text-centered">
                        Histogram of the track lengths.
                    </h2>
                    <img src="static/images/histogram_lane_distribution.svg" alt="infrastructure_sensors"/>
                    <h2 class="subtitle has-text-centered">
                        Lane distribution of all labeled objects on the highway.
                    </h2>
                    <img src="static/images/bar_chart_speed_all_drives.svg" alt="infrastructure_sensors"/>
                    <h2 class="subtitle has-text-centered">
                        <b>Visualization of speed values for each labeled category.</b> We show the average and maximum
                        speed values for all categories. The average speed is 107 km/h in the dataset.
                    </h2>
                    <img src="static/images/pie_chart_accident_types.svg" alt="infrastructure_sensors" width="50%"/>
                    <h2 class="subtitle has-text-centered">
                        <b>Distribution of accident types.</b>
                    </h2>
                    <img src="static/images/heatmap.jpg" alt="infrastructure_sensors" style="border:2px solid black;"/>
                    <h2 class="subtitle has-text-centered">
                        <b>Heatmap visualization of traffic participant locations.</b> The left lane on the highway
                        towards the north direction indicates a high traffic volume.
                    </h2>
                </center>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                <h2 class="title is-3">Quantitative Evaluation Results</h2>
                <img src="static/images/evaluation.png" alt="infrastructure_sensors" style="border:2px solid black;"/>
                <h2 class="subtitle">
                    <b>Quantitative evaluation results</b> of the accident detection module (rule-based approach) over a
                    128-day monitoring period. The statistics include the detected vehicles, maneuvers, scenarios, and
                    accident events.
                </h2>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                <h2 class="title is-3">Qualitative Evaluation Results</h2>
                <center>
                    <img src="static/images/qualitative_result_rule_based_approach.png" alt="infrastructure_sensors"
                         width="50%" style="border:2px solid black;"/>
                    <h2 class="subtitle">
                        <b>Qualitative visualization results of our accident detection framework on the TUMTraf-Accid3nD test
                            set.</b> The rule-based approach detected a rear-end collision.
                    </h2>
                    <img src="static/images/qualitative_result_learning_based_approach.png" alt="infrastructure_sensors"
                         width="50%" style="border:2px solid black;"/>
                    <h2 class="subtitle">
                        <b>Qualitative visualization results of our accident detection framework on the TUMTraf-Accid3nD test
                            set.</b> The learning-based approach detected a car crash with a confidence threshold of
                        0.8.
                    </h2>
                </center>
            </div>
        </div>
    </div>
</section>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            Sequence 01:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s01_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 02:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s02_scaled.mp4" type="video/mp4">
            </video>
            Sequence 03:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s03_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 04:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s04_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 05:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s05_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 06:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s06_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 07:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s07_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 08:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s08_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 09:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s09_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 10:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s10_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 11:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s11_2x2_scaled.mp4" type="video/mp4">
            </video>
            Sequence 12:
            <video poster="" autoplay controls muted loop height="100%" style="border:2px solid black;">
                <source src="static/videos/r09_s12_2x2_scaled.mp4" type="video/mp4">
            </video>
        </div>
    </div>
</section>

<section class="hero">
    <div class="hero">
        <h3 class="title is-8 has-text-centered">Experiments</h3>
        <center>
            <div class="container is-centered">
                <table class="table is-centered center">
                    <thead>
                    <tr>
                        <th>Event</th>
                        <th colspan="2" align="center">Runtime</th>
                    </tr>
                    <tr>
                        <th></th>
                        <th>Rule-based</th>
                        <th>Learning-based</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>Sequence S01, part I</td>
                        <td>8.63</td>
                        <td>484.69</td>
                    </tr>
                    <tr>
                        <td>Sequence S01, part II</td>
                        <td>6.60</td>
                        <td>474.69</td>
                    </tr>
                    <tr>
                        <td>Sequence S13, part I</td>
                        <td>5.02</td>
                        <td>240.43</td>
                    </tr>
                    <tr>
                        <td>Sequence S13, part II</td>
                        <td>5.33</td>
                        <td>248.16</td>
                    </tr>
                    <tr>
                        <td>Avg. (with 2 cameras)</td>
                        <td>5.17</td>
                        <td>244.30</td>
                    </tr>
                    <tr>
                        <td>Avg. (with 4 cameras)</td>
                        <td>7.61</td>
                        <td>479.69</td>
                    </tr>
                    <tr>
                        <td>Average (overall)</td>
                        <td><strong>6.39</strong></td>
                        <td>361.99</td>
                    </tr>
                    </tbody>
                </table>
                <h5 class="is-3 has-text-centered">
                    Runtime comparison of <strong>AccidentDet3D</strong> on a 15 minute long traffic recording
                    containing 20,000 frames.</b> We compare our
                    rule-based and learning-based accident detection based on the runtime (in seconds). We ablate on the
                    number of cameras used for the detection task to show the scalability of our approach.
                </h5>
            </div>
        </center>
    </div>
</section>

<section class="hero">
    <div class="hero">
        <h3 class="title is-8 has-text-centered">Benchmark</h3>
        <center>
            <div class="container is-centered">
                <table class="table is-centered center">
                    <thead>
                    <tr>
                        <th>Method</th>
                        <th colspan="3" align="center">Accuracy</th>
                        <th colspan="2" align="center">Runtime [s]</th>
                    </tr>
                    <tr>
                        <th></th>
                        <th>Precision</th>
                        <th>Recall</th>
                        <th>F1-Score</th>
                        <th>2 cameras</th>
                        <th>4 cameras</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>Rule-based Approach</td>
                        <td>1.000</td>
                        <td>0.500</td>
                        <td>0.667</td>
                        <td>0.086</td>
                        <td>0.127</td>
                    </tr>
                    <tr>
                        <td>Learning-based Approach</td>
                        <td>0.800</td>
                        <td>1.000</td>
                        <td>0.889</td>
                        <td>4.072</td>
                        <td>7.995</td>
                    </tr>

                    </tbody>
                </table>
                <h5 class="is-3 has-text-centered">
                    Accident detection results of <strong>AccidentDet3D</strong> on our <b>TUMTraf-Accid3nD test set.</b> We
                    compare our rule-based and learning-based accident detection approach on our test set.
                </h5>
            </div>
        </center>
    </div>
</section>

<!--<section class="hero">-->
<!--    <div class="hero">-->
<!--        <h3 class="title is-8 has-text-centered">Benchmark</h3>-->
<!--        <div class="container is-centered">-->
<!--            <table class="table is-centered center">-->
<!--                <thead>-->
<!--                <tr>-->
<!--                    <th>Config</th>-->
<!--                    <th></th>-->
<!--                    <th>BEV mAP</th>-->
<!--                    <th></th>-->
<!--                    <th>3D mAP</th>-->
<!--                    <th></th>-->
<!--                    <th></th>-->
<!--                </tr>-->
<!--                <tr>-->
<!--                    <th>Domain</th>-->
<!--                    <th>Modality</th>-->
<!--                    <th>&nbsp;</th>-->
<!--                    <th>Easy</th>-->
<!--                    <th>Moderate</th>-->
<!--                    <th>Hard</th>-->
<!--                    <th>Average</th>-->
<!--                </tr>-->
<!--                </thead>-->
<!--                <tbody>-->
<!--                <tr>-->
<!--                    <td>TBD</td>-->
<!--                    <td>TBD</td>-->
<!--                    <td>TBD</td>-->
<!--                    <td>TBD</td>-->
<!--                    <td>TBD</td>-->
<!--                    <td>TBD</td>-->
<!--                    <td>TBD</td>-->
<!--                </tr>-->

<!--                </tbody>-->
<!--            </table>-->
<!--            <h5 class="is-3 has-text-centered">-->
<!--                Evaluation results (BEV mAP and 3D mAP) of <strong>AccidentDet3D</strong> on our <br>-->
<!--                Accid3nD test set.-->
<!--            </h5>-->
<!--        </div>-->
<!--    </div>-->
<!--</section>-->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content has-text-centered">
                    <p>
                        The <strong>TUMTraf-Accid3nD</strong> dataset is licensed under <a
                            href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA
                        4.0</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>

</html>
